{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Assignment 2\n",
    "### Data Mining 7331 Section 403\n",
    "---\n",
    "- Brian Coari\n",
    "- Stephen Merritt\n",
    "- Cory Thigpen\n",
    "- Quentin Thomas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "#### Section 1\n",
    "\n",
    "- (10pts) Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering\n",
    "Based on results from the Mini-lab, we investigated possible features that will improve the model's ability to select *True Positives* or actual **Podium Finishers**.  In the mini-lab we added `Population_Prop` which is world populatio proportion for each represented country with the thought that larger countries would produce a higher proportion of **Podium Finishers**.  We also added the previously calclated `BMI` feature to the dataset.  A final addition for this assignment, was to reward prior medal winners when they compete in future Olympic Games.  This feature, `Previous_Medals`, is a binary feature that establishes an athlete as **Podium Finisher** caliber. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#Combine existing data with population data from:\n",
    "#https://data.world/worldbank/total-population-per-country/file/POP_TOTAL_DS2_en_v2.csv\n",
    "df = pd.read_csv('~/olympics/data/athletes_cleaned_merged.csv')\n",
    "df_pop = pd.read_csv('~/olympics/data/POP_TOTAL.csv', encoding = \"ISO-8859-1\")\n",
    "df_pop = df_pop[['Country Code', '2015']]\n",
    "#Change all Russia NOCs\n",
    "df['NOC'] = np.where(df['NOC'] == 'EUN', 'RUS', df['NOC'])\n",
    "df['NOC'] = np.where(df['NOC'] == 'URS', 'RUS', df['NOC'])\n",
    "#Change all Australia NOCs\n",
    "df['NOC'] = np.where(df['NOC'] == 'ANZ', 'AUS', df['NOC'])\n",
    "#Change all German NOCs\n",
    "df['NOC'] = np.where(df['NOC'] == 'FRG', 'GER', df['NOC'])\n",
    "df['NOC'] = np.where(df['NOC'] == 'GDR', 'GER', df['NOC'])\n",
    "df['NOC'] = np.where(df['NOC'] == 'SAA', 'GER', df['NOC'])\n",
    "#Change all Congo NOCs\n",
    "df['NOC'] = np.where(df['NOC'] == 'CGO', 'COD', df['NOC'])\n",
    "#Change all Czech NOCs\n",
    "df['NOC'] = np.where(df['NOC'] == 'BOH', 'CZE', df['NOC'])\n",
    "df['NOC'] = np.where(df['NOC'] == 'TCH', 'CZE', df['NOC'])\n",
    "#Change all Yemen NOCs\n",
    "df['NOC'] = np.where(df['NOC'] == 'YAR', 'YEM', df['NOC'])\n",
    "df['NOC'] = np.where(df['NOC'] == 'YMD', 'YEM', df['NOC'])\n",
    "#Change all Greek NOCs\n",
    "df['NOC'] = np.where(df['NOC'] == 'CRT', 'GRE', df['NOC'])\n",
    "#Change all Zimbabwe NOCs\n",
    "df['NOC'] = np.where(df['NOC'] == 'RHO', 'ZIM', df['NOC'])\n",
    "#Change all Malaysia NOCs\n",
    "df['NOC'] = np.where(df['NOC'] == 'MAL', 'MAS', df['NOC'])\n",
    "df['NOC'] = np.where(df['NOC'] == 'NBO', 'MAS', df['NOC'])\n",
    "#Change all Vietnam NOCs\n",
    "df['NOC'] = np.where(df['NOC'] == 'VNM', 'VIE', df['NOC'])\n",
    "#Change all Trinidad and Tobego NOCs\n",
    "df['NOC'] = np.where(df['NOC'] == 'WIF', 'TTO', df['NOC'])\n",
    "#Change all Trinidad and Tobego NOCs\n",
    "df['NOC'] = np.where(df['NOC'] == 'UAR', 'SYR', df['NOC'])\n",
    "#Change all Serbian NOCs\n",
    "df['NOC'] = np.where(df['NOC'] == 'SCG', 'YUG', df['NOC'])\n",
    "df['NOC'] = np.where(df['NOC'] == 'SRB', 'YUG', df['NOC'])\n",
    "#Change all Canada NOCs\n",
    "df['NOC'] = np.where(df['NOC'] == 'NFL', 'CAN', df['NOC'])\n",
    "df_pop.rename(index = str, columns = {'Country Code' : 'NOC', '2015': '2015 Population'}, inplace = True)\n",
    "df_pop_merge = pd.merge(left = df, right = df_pop, how = 'left', on=['NOC'], left_index=False)\n",
    "df_pop_merge.loc[df_pop_merge['Country'] == 'Taiwan', '2015 Population'] =  23485755\n",
    "df_pop_merge.loc[df_pop_merge['Country'] == 'Saint Vincent', '2015 Population'] =  109643\n",
    "df_pop_merge.loc[df_pop_merge['Country'] == 'Palestine', '2015 Population'] =  4817000\n",
    "df_pop_merge.loc[df_pop_merge['Country'] == 'Cook Islands', '2015 Population'] =  17459\n",
    "df_pop_merge.loc[df_pop_merge['Country'] == 'Eritrea', '2015 Population'] =  4846976\n",
    "df_pop_merge.loc[df_pop_merge['Country'] == 'Saint Kitts', '2015 Population'] =  54821\n",
    "df_pop_mergena = df_pop_merge[df_pop_merge['2015 Population'].isnull()]\n",
    "#Drop the 106 observations not associated with a country \n",
    "df_pop_merge = df_pop_merge.dropna(how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caluclate the population proportion for each country\n",
    "df_pop = df_pop_merge.groupby(['Country'])['2015 Population'].mean().reset_index()\n",
    "df_pop['Population_Prop'] = df_pop['2015 Population'].transform(lambda x: x / (x.sum()))\n",
    "\n",
    "#Calculate BMI for each athlete\n",
    "def calculate_bmi(df):\n",
    "    return (df[1]/(df[0] * df[0])) * 10000\n",
    "df_pop_merge['BMI'] = df_pop_merge[['Height', 'Weight']].apply(calculate_bmi, axis=1)\n",
    "\n",
    "#Merge dataframes with Population Propotion information\n",
    "df_pop_merge = pd.merge(left = df_pop_merge, right = df_pop, how = 'left', on=['Country'], \n",
    "                        left_index=False)\n",
    "#Create Binary Medal, Season, and Sex Classes\n",
    "df_pop_merge['Medal'] = np.where(df_pop_merge.Medal == 'No Medal', 0, 1)\n",
    "df_pop_merge['Season'] = np.where(df_pop_merge.Season == 'Summer', 1, 0)\n",
    "df_pop_merge['Sex'] = np.where(df_pop_merge.Sex == 'M', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Steve/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        Name  Year  Medal_Count  \\\n",
      "0    Gabrielle Marie \"Gabby\" Adcock (White-)  2016            0   \n",
      "1        Eleonora Margarida Josephina Scmitt  1948            0   \n",
      "2                            Jean Hauptmanns  1912            0   \n",
      "3    Luis ngel Fernando de los Santos Grossi  1948            0   \n",
      "4    Luis ngel Fernando de los Santos Grossi  1952            0   \n",
      "\n",
      "   Previous_Medals  \n",
      "0                0  \n",
      "1                0  \n",
      "2                0  \n",
      "3                0  \n",
      "4                0  \n",
      "CPU times: user 1h 19min 47s, sys: 26.6 s, total: 1h 20min 14s\n",
      "Wall time: 1h 20min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prev_medals = df_pop_merge.groupby(['Name', 'Year'])['Medal'].sum()\n",
    "prev_medals = prev_medals.reset_index()\n",
    "prev_medals.columns = ['Name', 'Year', 'Medal_Count']\n",
    "prev_medals['Previous_Medals'] = prev_medals['Medal_Count']\n",
    "\n",
    "for i in range(2, len(prev_medals)):\n",
    "    if prev_medals.iloc[i]['Name'] == prev_medals.iloc[i-1]['Name']:\n",
    "        if prev_medals.iloc[i-1]['Medal_Count'] != 0:\n",
    "            prev_medals['Previous_Medals'].iloc[i] = 1\n",
    "        else:\n",
    "            prev_medals['Previous_Medals'].iloc[i] = 0\n",
    "    else:\n",
    "        prev_medals['Previous_Medals'].iloc[i] = 0\n",
    "print(prev_medals.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge prev_medals into existing pop_merge dataset to add Previous_Medals feature\n",
    "df_prev_medal_merge = pd.merge(left = df_pop_merge, right = prev_medals, how = 'left', on = ['Name', 'Year'])\n",
    "df_pop_merge = df_prev_medal_merge[['Country', 'Sex', 'Age', 'Height', 'Weight', 'Year', 'Season', 'Sport', 'Event', 'BMI', \n",
    "                             'Population_Prop', 'Medal', 'Previous_Medals']]\n",
    "\n",
    "#Write dataframe to .csv file in order to preserve the data and avoid Feature Engineering code in future iterations \n",
    "df_pop_merge.to_csv('~/olympics/data/pop_merge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 266848 entries, 0 to 266847\n",
      "Data columns (total 13 columns):\n",
      "Unnamed: 0         266848 non-null int64\n",
      "Country            266848 non-null object\n",
      "Sex                266848 non-null int64\n",
      "Age                266848 non-null int64\n",
      "Height             266848 non-null int64\n",
      "Weight             266848 non-null int64\n",
      "Year               266848 non-null int64\n",
      "Season             266848 non-null int64\n",
      "Sport              266848 non-null object\n",
      "Event              266848 non-null object\n",
      "BMI                266848 non-null float64\n",
      "Population_Prop    266848 non-null float64\n",
      "Previous_Medals    266848 non-null int64\n",
      "dtypes: float64(2), int64(8), object(3)\n",
      "memory usage: 26.5+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Create X and y vectors for sklearn pre-processing steps.\n",
    "df = pd.read_csv('~/olympics/data/pop_merge.csv')\n",
    "\n",
    "y = df['Medal']\n",
    "X = df.drop(['Medal'], axis = 1)\n",
    "\n",
    "print(X.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Unnamed: 0            Sex            Age         Height  \\\n",
      "count  266848.000000  266848.000000  266848.000000  266848.000000   \n",
      "mean   133423.500000       0.722786      25.330338     175.420948   \n",
      "std     77032.526654       0.447624       5.801661       9.869066   \n",
      "min         0.000000       0.000000      10.000000     127.000000   \n",
      "25%     66711.750000       0.000000      21.000000     169.000000   \n",
      "50%    133423.500000       1.000000      24.000000     176.000000   \n",
      "75%    200135.250000       1.000000      28.000000     182.000000   \n",
      "max    266847.000000       1.000000      72.000000     226.000000   \n",
      "\n",
      "              Weight           Year         Season            BMI  \\\n",
      "count  266848.000000  266848.000000  266848.000000  266848.000000   \n",
      "mean       71.017542    1979.016369       0.818185      22.904691   \n",
      "std        13.330644      29.560519       0.385693       2.718862   \n",
      "min        25.000000    1896.000000       0.000000       8.360954   \n",
      "25%        62.000000    1960.000000       1.000000      21.258503   \n",
      "50%        71.000000    1988.000000       1.000000      22.783309   \n",
      "75%        78.000000    2004.000000       1.000000      24.163265   \n",
      "max       214.000000    2016.000000       1.000000      63.901580   \n",
      "\n",
      "       Population_Prop  Previous_Medals       ...         Sport_Table Tennis  \\\n",
      "count    266848.000000    266848.000000       ...              266848.000000   \n",
      "mean          0.013419         0.076883       ...                   0.007150   \n",
      "std           0.028553         0.266406       ...                   0.084256   \n",
      "min           0.000001         0.000000       ...                   0.000000   \n",
      "25%           0.001362         0.000000       ...                   0.000000   \n",
      "50%           0.005258         0.000000       ...                   0.000000   \n",
      "75%           0.011266         0.000000       ...                   0.000000   \n",
      "max           0.181422         1.000000       ...                   1.000000   \n",
      "\n",
      "       Sport_Taekwondo   Sport_Tennis  Sport_Trampolining  Sport_Triathlon  \\\n",
      "count    266848.000000  266848.000000        266848.00000    266848.000000   \n",
      "mean          0.002271       0.010721             0.00057         0.001982   \n",
      "std           0.047600       0.102988             0.02386         0.044480   \n",
      "min           0.000000       0.000000             0.00000         0.000000   \n",
      "25%           0.000000       0.000000             0.00000         0.000000   \n",
      "50%           0.000000       0.000000             0.00000         0.000000   \n",
      "75%           0.000000       0.000000             0.00000         0.000000   \n",
      "max           1.000000       1.000000             1.00000         1.000000   \n",
      "\n",
      "       Sport_Tug-Of-War  Sport_Volleyball  Sport_Water Polo  \\\n",
      "count     266848.000000     266848.000000     266848.000000   \n",
      "mean           0.000637          0.012756          0.014375   \n",
      "std            0.025232          0.112221          0.119032   \n",
      "min            0.000000          0.000000          0.000000   \n",
      "25%            0.000000          0.000000          0.000000   \n",
      "50%            0.000000          0.000000          0.000000   \n",
      "75%            0.000000          0.000000          0.000000   \n",
      "max            1.000000          1.000000          1.000000   \n",
      "\n",
      "       Sport_Weightlifting  Sport_Wrestling  \n",
      "count        266848.000000    266848.000000  \n",
      "mean              0.014709         0.026776  \n",
      "std               0.120385         0.161427  \n",
      "min               0.000000         0.000000  \n",
      "25%               0.000000         0.000000  \n",
      "50%               0.000000         0.000000  \n",
      "75%               0.000000         0.000000  \n",
      "max               1.000000         1.000000  \n",
      "\n",
      "[8 rows x 792 columns]\n",
      "count    266848.000000\n",
      "mean          0.147841\n",
      "std           0.354943\n",
      "min           0.000000\n",
      "25%           0.000000\n",
      "50%           0.000000\n",
      "75%           0.000000\n",
      "max           1.000000\n",
      "Name: Medal, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "X = pd.get_dummies(X, columns = ['Country', 'Event', 'Sport'])\n",
    "print(X.describe())\n",
    "print(y.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "#Standardize X\n",
    "sc = StandardScaler()\n",
    "sc.fit(X)\n",
    "X_std = sc.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 2\n",
    "\n",
    "- (5pts) Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling and Evaluation\n",
    "\n",
    "#### Section 1\n",
    "\n",
    "- (10 pts) Choose and explain your evaluation metrics that you will use (i.e., accuracy, precision, recall, F-measure, or any metric we have discussed). Why are the measure(s) appropriate for analyzing the results of your modeling? Give a detailed explanation backing up any assertions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 2\n",
    "\n",
    "- (10pts) Choose the method you will use for dividing your data into training and testing splits (i.e., are you using Stratified 10-fold cross validation? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate. For example, if you are using time series data then you should be using continuous training and testing sets across time.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 3\n",
    "\n",
    "- (20pts) Create three different classification/regression models for each task (e.g., random forest, KNN, and SVM for task one and the same or different algorithms for task two). Two modeling techniques must be new (but the third could be SVM or logistic regression). Adjust parameters as appropriate to increase generalization performance using your chosen metric. You must investigate different parameters of the algorithms!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1 (Classify Podium Finishers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifer Estimator created and edited from Professor Drew's NC school repository\n",
    "def EvaluateClassifierEstimator2(classifierEstimator, X, y, cv):\n",
    "    \n",
    "    #Perform cross validation \n",
    "    from sklearn.model_selection import cross_val_predict\n",
    "    predictions = cross_val_predict(classifierEstimator, X, y, cv=cv)\n",
    "    \n",
    "    #model evaluation \n",
    "    from sklearn.metrics import classification_report, confusion_matrix, accuracy_score \n",
    "    \n",
    "    #pass true test set values and predictions to classification_report\n",
    "    classReport = classification_report(y, predictions)\n",
    "    confMat = confusion_matrix(y, predictions)\n",
    "    acc = accuracy_score(y, predictions)\n",
    "\n",
    "    \n",
    "    print(classReport)\n",
    "    print(confMat)\n",
    "    print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logisitic regression 10-fold cross-validation \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "lr = LogisticRegression()\n",
    "\n",
    "\n",
    "parameters = { 'penalty':['l2','l1']\n",
    "              ,'C': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "              ,'class_weight': ['balanced','none']\n",
    "              ,'random_state': [0]\n",
    "              ,'solver': ['sag', 'saga']\n",
    "              ,'max_iter':[100,500]\n",
    "             }\n",
    "\n",
    "#Create a grid search object using the  \n",
    "regGS = GridSearchCV(estimator=lr\n",
    "                   , n_jobs=-1 #Use all possible cores to run jobs in parallel\n",
    "                   , verbose=True\n",
    "                   , param_grid=parameters\n",
    "                   , cv=cv # KFolds = 10\n",
    "                   , scoring='f1')\n",
    "\n",
    "#Perform hyperparameter search to find the best combination of parameters for our data\n",
    "regGS.fit(X_std, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf = GS.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EvaluateClassifierEstimator2(lr_clf, X_std, y, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf = lr_clf.fit(X_std, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "gs_lr_model = 'gs_lr_model.joblib.pkl'\n",
    "_ = joblib.dump(lr_clf, gs_lr_model, compress = 9)\n",
    "#This will save the model in your working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = joblib.load(gs_lr_model)\n",
    "clf2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 4\n",
    "\n",
    "- (10pts) Analyze the results using your chosen method of evaluation. Use visualizations of the results to bolster the analysis. Explain any visuals and analyze why they are interesting to someone that might use this model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 5\n",
    "\n",
    "- (10pts) Discuss the advantages of each model for each classification task, if any. If there are not advantages, explain why. Is any model better than another? Is the difference significant with 95% confidence? Use proper statistical comparison methods. You must use statistical comparison techniques—be sure they are appropriate for your chosen method of validation as discussed in unit 7 of the course.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 6\n",
    "\n",
    "- (10pts) Which attributes from your analysis are most important? Use proper methods discussed in class to evaluate the importance of different attributes. Discuss the results and hypothesize about why certain attributes are more important than others for a given classification task.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment\n",
    "\n",
    "- (5pts) How useful is your model for interested parties (i.e., the companies or organizations that might want to use it for prediction)? How would you measure the model's value if it was used by these parties? How would your deploy your model for interested parties? What other data should be collected? How often would the model need to be updated, etc.? \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exceptional Work\n",
    "\n",
    "- (10pts) You have free reign to provide additional analyses. One idea: grid search parameters in a parallelized fashion and visualize the performances across attributes. Which parameters are most significant for making a good model for each classification algorithm?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
